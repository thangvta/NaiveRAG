{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaviate Vector DB Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container rag-comprehensive-weaviate-1  Starting\n",
      "Error response from daemon: driver failed programming external connectivity on endpoint rag-comprehensive-weaviate-1 (5e50f0752e195469cde8b8dcd29bfc0ba2c8a7f0d2a47aafd74277baa912e587): Bind for 0.0.0.0:50051 failed: port is already allocated\n"
     ]
    }
   ],
   "source": [
    "!docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\weaviate\\warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Create weaviate client\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "client = weaviate.Client(\n",
    "    url = \"http://localhost:8080\",  # Replace with your endpoint\n",
    "    additional_headers = {\n",
    "        \"X-Google-Api-Key\": google_api_key # Replace with your inference API key\n",
    "    }\n",
    ")\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# Load documents\n",
    "documents=SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = Gemini(models='gemini-pro', api_key=google_api_key)\n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "\n",
    "# construct vector store \n",
    "vector_store = WeaviateVectorStore(weaviate_client=client)\n",
    "\n",
    "# set up storage for embeddings\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# set up the index \n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query with Default Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The author wrote short stories and programmed on an IBM 1401 computer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "display_response(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "\n",
    "eval_dataset = LabelledRagDataset.from_json(\"./eval_data/eval_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopCandidateException",
     "evalue": "index: 0\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: HIGH\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: NEGLIGIBLE\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopCandidateException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m generated_answers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m----> 4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m  \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     generated_answers\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:187\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    184\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    186\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 187\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:180\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    178\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    179\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 180\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    181\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    182\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    183\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    184\u001b[0m         ],\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    188\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    189\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     39\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     40\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     41\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:160\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[0;32m    161\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    166\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    167\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:214\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[1;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    213\u001b[0m             StructuredRefineResponse,\n\u001b[1;32m--> 214\u001b[0m             program(\n\u001b[0;32m    215\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[0;32m    216\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    217\u001b[0m             ),\n\u001b[0;32m    218\u001b[0m         )\n\u001b[0;32m    219\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:68\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     66\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m     71\u001b[0m     )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\llms\\llm.py:253\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[0;32m    252\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 253\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:93\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[0;32m     85\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m     86\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m     87\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m         },\n\u001b[0;32m     92\u001b[0m     )\n\u001b[1;32m---> 93\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_index\\llms\\gemini\\base.py:162\u001b[0m, in \u001b[0;36mGemini.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;241m*\u001b[39mhistory, next_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(chat_message_to_gemini, merged_messages)\n\u001b[0;32m    161\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mstart_chat(history\u001b[38;5;241m=\u001b[39mhistory)\n\u001b[1;32m--> 162\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_from_gemini_response(response)\n",
      "File \u001b[1;32mc:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\google\\generativeai\\generative_models.py:384\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    380\u001b[0m         glm\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mFINISH_REASON_UNSPECIFIED,\n\u001b[0;32m    381\u001b[0m         glm\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mSTOP,\n\u001b[0;32m    382\u001b[0m         glm\u001b[38;5;241m.\u001b[39mCandidate\u001b[38;5;241m.\u001b[39mFinishReason\u001b[38;5;241m.\u001b[39mMAX_TOKENS,\n\u001b[0;32m    383\u001b[0m     ):\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mStopCandidateException(response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_sent \u001b[38;5;241m=\u001b[39m content\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_received \u001b[38;5;241m=\u001b[39m response\n",
      "\u001b[1;31mStopCandidateException\u001b[0m: index: 0\nfinish_reason: SAFETY\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: HIGH\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: NEGLIGIBLE\n}\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.from_pandas(eval_dataset.to_pandas())\n",
    "generated_answers = []\n",
    "for example in data:\n",
    "    answer =  query_engine.query(example['query'])\n",
    "    generated_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'reference_contexts', 'reference_answer', 'reference_answer_by', 'query_by'],\n",
      "    num_rows: 44\n",
      "})\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.from_pandas(eval_dataset.to_pandas())\n",
    "print(data)\n",
    "question = [x for x in data['query']]\n",
    "contexts = [x for x in data['reference_contexts']]\n",
    "print(data['reference_answer'])\n",
    "ground_truth = [[x] for x in data['reference_answer']]\n",
    "\n",
    "generated_answers = [str(x) for x in generated_answers]\n",
    "\n",
    "formatted_data =  Dataset.from_dict({\n",
    "    \"question\": question,\n",
    "    \"contexts\": contexts,\n",
    "    \"answer\": generated_answers,\n",
    "    \"ground_truths\": ground_truth\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Iterate over the dictionary items and write them to the file\n",
    "    for key, value in formatted_data.items():\n",
    "        file.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thangta1\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatted_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m evaluate(dataset\u001b[38;5;241m=\u001b[39m\u001b[43mformatted_data\u001b[49m,metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'formatted_data' is not defined"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(dataset=formatted_data,metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
